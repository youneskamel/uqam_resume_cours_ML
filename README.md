# MIGD UQAM : resum√©s des cours ML üéì

Un repo qui contiendra un document markdown qui resume ce quel'on a appris durant les cours d'apprentissage automatique que nous avons suivi a l'uqam. Ceux ci sont inf7710, inf8100 et inf7370.

## INF7710 ‚õèÔ∏è

## INF8100 üìä

Le cours INF8100, intitul√© concepts et techniques en fouille et exploration de donn√©es. Ce cours est une introduction g√©n√©rale √† la science des donn√©es, qui couvre √©galement les  math√©matiques n√©cessaire √† l'√©tude de ce sujet.

La premi√®re s√©ance de cours a servi √† d√©finir ce qu‚Äôest la science des donn√©es et quels sont ces buts. Une phrase du cours qui semble bien incarner ce qu‚Äôest la science des donn√©es est la suivante :

‚ÄúUn scientifique de donn√©es a pour r√¥le d‚Äôextraire des informations exploitables (actionable insights) des donn√©es"

Lors de ce cours nous avons aussi discut√© des outils utilis√©s par les scientifiques de donn√©es tels que python, R et les Jupyter notebooks. Nous avons √©galement d√©fini les √©tapes d‚Äôun projet en science de donn√©es (d√©finition de la question , s√©lectionner des donn√©es, pr√©-traitement, transformation, fouille, evaluation et validation)

Les cours suivants nous permirent de revoir les math√©matiques pr√©alables √† la science des donn√©es, soit l'alg√®bre lin√©aire et les statistiques. Nous avons revu les additions et multiplication de vecteurs/matrices, les valeurs propres et la d√©composition en valeurs singuli√®res notamment, pour l'alg√®bre lin√©aire.

Pour les statistiques, nous avons vu la loi des grands nombres, les tests statistiques, les p-values, les lois de probabilit√©s, etc‚Ä¶

Puis, le chapitre/cours suivant fut une √©tude d‚Äôun projet de science des donn√©es de bout en bout. A l‚Äôaide d‚Äôun notebook Jupyter nous avons men√© un petit projet de science des donn√©es en classe tout en expliquant chaque √©tape. Cela nous a permis de voir en d√©tail le nettoyage des donn√©es, la transformation des donn√©es et la visualisation par exemple.

Nous avons ensuite abord√© la r√©gression, une technique statistique qui permet de cr√©er des mod√®les pour faire de la pr√©diction et de la classification. Nous avons vu la r√©gression lin√©aire, logistique et polynomiale. Nous avons √©galement abord√© d'autres types de mod√®les tels que les SVM. Ce chapitre est une introduction aux fondamentaux de l'apprentissage automatique supervis√©, il fut donc crucial pour nous.

Les notions abord√©es ensuite sont celles de la r√©duction de la dimensionnalit√© et celle du partitionnement. Nous avons notamment trait√© le sujet de l‚Äôanalyse en composante principale (PCA/ACP) et de la LDA (linear discriminant analysis). Ensuite, nous avons appris les algorithmes k-means, les m√©langes gaussiens et l‚Äôalgorithme Expectation Maximization. Nous avons donc √©t√© introduits √† diff√©rents algorithmes centraux de l‚Äôapprentissage non-supervise. 

## INF7370 ü§ñ

Le cours INF7710, intitul√© apprentissage automatique, est un cours d‚Äôintroduction g√©n√©rale √† l‚Äôapprentissage automatique, couvrant la majorit√© des sujets du domaine. Le cours nous a permis √† la fois de mettre en pratique des algorithmes d'apprentissage automatique, mais aussi d'apprendre les math√©matiques et les algorithmes derri√®re diff√©rents mod√®les.

Le premier cours commence par d√©finir l'apprentissage automatique, soit ; 

‚Äúll s‚Äôagit de concevoir des algorithmes capables, √† partir d‚Äôun nombre important d‚Äôexemples, d‚Äôen assimiler la nature afin de pouvoir appliquer ce qu‚Äôils ont ainsi appris aux cas futurs.‚Äù

En d‚Äôautres termes, l‚Äôapprentissage automatique est un ensemble de techniques permettant √† un programme de r√©aliser un t√¢che et de s'y am√©liorer sans √™tre programm√© explicitement. Nous avons √©galement abord√© la distinction entre l‚Äôapprentissage supervis√© et non-supervis√© (c.a.d. la pr√©sence d'√©tiquettes ou labels).

Le deuxi√®me cours fut une introduction aux arbres de d√©cision. Ce terme caract√©rise un type de mod√®le d‚Äôapprentissage automatique dont la repr√©sentation est une structure de donn√©es d‚Äôarbre. Ces arbres permettent de prendre des d√©cisions en r√©pondant aux questions contenues dans leurs feuilles et en suivant la branche associ√©e √† la r√©ponse. Nous avons notamment vu l'algorithme de Hunt pour entra√Æner les arbres de d√©cision.

Le troisi√®me cours porta sur l'apprentissage d‚Äôensembles. Cette technique consiste √† apprendre diff√©rents mod√®les d‚Äôapprentissage automatique et √† les combiner afin d‚Äôobtenir une meilleure performance. Il existe notamment, le bagging, le pasting et le boosting, diff√©rentes formes d‚Äôapprentissage d‚Äôensemble.

Ensuite, nous avons abord√© la classification bay√©sienne na√Øve. Cette technique permet de faire de la classification en utilisant la formule de Bayes :


P(A|B) = P(B|A).P(A) / P(B) 

Nous avons ensuite expliqu√© les r√©seaux bay√©siens. Ces r√©seaux permettent de repr√©senter les probabilit√©s conditionnelles de divers √©v√©nements de fa√ßon compacte. En utilisant, √©galement la formule de Bayes, ils permettent de faire de la pr√©diction et de la classification.

Les s√©ances qui suivirent furent d√©di√©es aux r√©seaux de neurones. Les r√©seaux de neurones artificiels sont un type de mod√®le d‚Äôapprentissage automatique dont la repr√©sentation est un r√©seau inspir√© vaguement des neurones humains.

Nous avons vu comment la r√©tro-propagation de Hinton est utilis√©e pour entra√Æner les r√©seaux de neurones en utilisant le gradient de l‚Äôerreur pour mesurer √† quel point chaque neurone est responsable de l‚Äôerreur. 

Cette approche a n√©anmoins un point faible, le probl√®me du gradient disparaissant. Nous avons abord√© les diff√©rents types d'initialisation ainsi que les diff√©rentes fonctions d'activation qui permettent de pallier ce probl√®me. Nous avons √©galement trait√© diff√©rents optimiseurs tels que adagrad, nesterov accelerated gradient ou ADAM. 

Nous avons ensuite vu diff√©rentes architectures de r√©seaux de neurones telles que les auto encodeurs, les r√©seaux de neurones convolutifs et les r√©seaux de neurones r√©currents.

Le cours s‚Äôest termin√© en abordant les th√®mes des SVM et enfin du partitionnement, une technique d‚Äôapprentissage supervis√©.
Pour les √©tudiants de la MIGD visant un profil orient√© ML engineer, ou Data scientist oriente ML, ce cours fut probablement le plus important de notre formation. Nous sommes reconnaissant qu‚Äôil ait √©t√© trait√© par un prof comp√©tent et de fa√ßon rigoureuse et profonde.
